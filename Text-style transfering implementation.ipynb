{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(EncoderNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, input_seq, hidden):\n",
    "        output, hidden = self.gru(input_seq, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, h0_size, h1_size, out_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.fcl1 = nn.Linear(input_size, h0_size)\n",
    "        self.fcl2 = nn.Linear(h0_size, h1_size)\n",
    "        self.out = nn.Linear(h1_size, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fcl1(x))\n",
    "        x = F.sigmoid(self.fcl2(x))\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        \n",
    "        y = F.softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device=device):\n",
    "        super(DecoderNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = output.view(-1, self.hidden_size)\n",
    "        output = self.softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(eldmitro): Implement DataLoader for texts\n",
    "# TODO(eldmitro): Implement train runner\n",
    "# TODO(eldmitro): Implement test runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '$SOS$'\n",
    "EOS_TOKEN = '$EOS$'\n",
    "special_tokens = (\n",
    "    SOS_TOKEN,\n",
    "    EOS_TOKEN,\n",
    ")\n",
    "\n",
    "def seq_one_hot_encode(sentence, dim_size):\n",
    "    N = sentence.size(0)\n",
    "    e_seq = torch.zeros(N, dim_size, dtype=torch.int8)\n",
    "    e_seq[torch.arange(N, dtype=torch.int64), sentence] = 1\n",
    "    \n",
    "    return e_seq\n",
    "\n",
    "def to_float(sentence, dim_size):\n",
    "    sentence = sentence.type(torch.float32)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        negative_examples_path,\n",
    "        positive_examples_path,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        device,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        super(SentimentDataset, self).__init__()\n",
    "        \n",
    "        # data reading from file\n",
    "        negative_data = []\n",
    "        with open(negative_examples_path) as input_stream:\n",
    "            negative_data = input_stream.readlines()\n",
    "\n",
    "        positive_data = []\n",
    "        with open(positive_examples_path) as input_stream:\n",
    "            positive_data = input_stream.readlines()\n",
    "        # ------------------------------------\n",
    "            \n",
    "        # data processing. Norm + tokenization\n",
    "        negative_data = [tokenizer.tokenize(text) for text in negative_data]\n",
    "        negative_data = [[stemmer.stem(word) for word in text] for text in negative_data]\n",
    "\n",
    "        positive_data = [tokenizer.tokenize(text) for text in positive_data]\n",
    "        positive_data = [[stemmer.stem(word) for word in text] for text in positive_data]\n",
    "        # ------------------------------------\n",
    "        \n",
    "        # dictionary processing\n",
    "        negative_dictionary = self._get_dictionary(negative_data)\n",
    "        positive_dictionary = self._get_dictionary(positive_data)\n",
    "\n",
    "        self.dictionary = negative_dictionary.union(positive_dictionary)\n",
    "        d = len(self.dictionary)\n",
    "        \n",
    "        for token in special_tokens:\n",
    "            self.dictionary.add(token)\n",
    "        assert d + len(special_tokens) == len(self.dictionary)\n",
    "        self.dict_size = len(self.dictionary)\n",
    "        \n",
    "        self.word2idx = dict(zip(\n",
    "            self.dictionary,\n",
    "            range(self.dict_size)\n",
    "        ))\n",
    "\n",
    "        self.idx2word = {value: key for key, value in self.word2idx.items()}\n",
    "        # ------------------------------------\n",
    "        \n",
    "        # target processing\n",
    "        self.styles = np.array([0] * len(negative_data) + [1] * len(positive_data))\n",
    "\n",
    "        data = negative_data + positive_data\n",
    "        assert len(data) == len(self.styles)\n",
    "        # ------------------------------------\n",
    "        \n",
    "        self._align_corpus(data, EOS_TOKEN, self._get_corpus_max_sent_len(data) + 1)\n",
    "        data = self._map_corpus(data, self.word2idx)\n",
    "        self.data = torch.tensor(data, dtype=torch.int64, device=device)\n",
    "        self.styles = torch.tensor(self.styles, dtype=torch.int8, device=device)\n",
    "        if transforms is None:\n",
    "            transforms = []\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "        \n",
    "    def _get_dictionary(self, corpus):\n",
    "        dictionary = set()\n",
    "        for text in corpus:\n",
    "            dictionary = dictionary.union(set(np.unique(text)))\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    def _get_corpus_max_sent_len(self, corpus):\n",
    "        text_max_length = 0\n",
    "        for text in corpus:\n",
    "            if len(text) > text_max_length:\n",
    "                text_max_length = len(text)\n",
    "                \n",
    "        return text_max_length\n",
    "    \n",
    "    def _align_corpus(self, corpus, align_token, max_length):\n",
    "        for i in range(len(corpus)):\n",
    "            corpus[i] += [align_token] * max(0, (max_length - len(corpus[i])))\n",
    "\n",
    "    def _map_corpus(self, corpus, mapper):\n",
    "        return [\n",
    "            [mapper[word] for word in text]\n",
    "            for text in corpus\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        t_data = self.data[index]\n",
    "        for transform in self.transforms:\n",
    "            t_data = transform(t_data, self.dict_size)\n",
    "            \n",
    "        return t_data, self.styles[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "\n",
    "negative_examples_filename = 'sentiment_negative.raw'\n",
    "positive_examples_filename = 'sentiment_positive.raw'\n",
    "\n",
    "negative_examples_path = os.path.join(data_path, negative_examples_filename)\n",
    "positive_examples_path = os.path.join(data_path, positive_examples_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_transforms = [\n",
    "    seq_one_hot_encode,\n",
    "    to_float\n",
    "]\n",
    "\n",
    "dataset = SentimentDataset(\n",
    "    negative_examples_path,\n",
    "    positive_examples_path,\n",
    "    tokenizer,\n",
    "    stemmer,\n",
    "    device,\n",
    "    seq_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset.dict_size\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = None, None\n",
    "for x, y in loader:\n",
    "    a, b = x, y\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hidden_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderNN(d, e_hidden_size, device)\n",
    "hidden_init = encoder.init_hidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = encoder.forward(x, hidden_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 21, 32])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4151,  0.1735, -0.2012,  0.0544, -0.1354,  0.0955,  0.0560,\n",
       "           0.1517, -0.0641, -0.1201,  0.3866, -0.0194,  0.0447, -0.1052,\n",
       "          -0.2339, -0.0320, -0.0233, -0.1572,  0.2762,  0.2425,  0.0557,\n",
       "           0.0448, -0.1122, -0.1686,  0.3160,  0.1523, -0.0463, -0.0954,\n",
       "          -0.0688, -0.2863,  0.3466,  0.0466],\n",
       "         [ 0.4180,  0.1745, -0.2068,  0.0593, -0.1351,  0.0971,  0.0598,\n",
       "           0.1493, -0.0623, -0.1143,  0.3882, -0.0188,  0.0410, -0.1022,\n",
       "          -0.2366, -0.0274, -0.0216, -0.1535,  0.2765,  0.2502,  0.0495,\n",
       "           0.0449, -0.1169, -0.1687,  0.3120,  0.1490, -0.0411, -0.0963,\n",
       "          -0.0717, -0.2824,  0.3487,  0.0447],\n",
       "         [ 0.4180,  0.1745, -0.2068,  0.0593, -0.1350,  0.0971,  0.0598,\n",
       "           0.1493, -0.0623, -0.1143,  0.3882, -0.0188,  0.0410, -0.1022,\n",
       "          -0.2366, -0.0275, -0.0216, -0.1535,  0.2766,  0.2502,  0.0495,\n",
       "           0.0449, -0.1169, -0.1687,  0.3120,  0.1490, -0.0410, -0.0963,\n",
       "          -0.0716, -0.2824,  0.3486,  0.0447]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_size = 32\n",
    "h1_size = 16\n",
    "out_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_batch = hidden.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    e_hidden_size,\n",
    "    h0_size,\n",
    "    h1_size,\n",
    "    out_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eldmitro/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/eldmitro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "output = mlp.forward(content_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hidden_size = e_hidden_size\n",
    "\n",
    "decoder = DecoderNN(d, d_hidden_size, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_content_batch = content_batch.view(1, content_batch.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_IDX = dataset.word2idx[SOS_TOKEN]\n",
    "output_init = torch.tensor([SOS_IDX] * batch_size)\n",
    "output_init = seq_one_hot_encode(output_init, d)\n",
    "output_init = output_init.view(batch_size, 1, -1)\n",
    "output_init = to_float(output_init, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "hiddens = []\n",
    "anss = []\n",
    "\n",
    "hidden = d_content_batch\n",
    "output = output_init\n",
    "ans = np.array([SOS_TOKEN] * batch_size)\n",
    "k = 0\n",
    "while (ans != EOS_TOKEN).any() and k < 3:\n",
    "    output, hidden = decoder.forward(output, hidden)\n",
    "    \n",
    "    k += 1\n",
    "    \n",
    "    ans = np.array([dataset.idx2word[int(t)] for t in output.argmax(axis=1)])\n",
    "    outputs.append(output)\n",
    "    hiddens.append(hidden)\n",
    "    anss.append(ans)\n",
    "    output = output.view(batch_size, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['stylist', 'stylist', 'stylist'], dtype='<U7'),\n",
       " array(['kill', 'kill', 'kill'], dtype='<U4'),\n",
       " array(['kill', 'kill', 'kill'], dtype='<U4')]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001],\n",
       "         [0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001],\n",
       "         [0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001],\n",
       "         [0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001],\n",
       "         [0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001],\n",
       "         [0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001],\n",
       "         [0.0001, 0.0001, 0.0002,  ..., 0.0002, 0.0001, 0.0001]],\n",
       "        grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentMultiDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        encoder_hidden_size,\n",
    "        mlp_h0_size,\n",
    "        mlp_h1_size,\n",
    "        device\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.mlp_h0_size = mlp_h0_size\n",
    "        self.mlp_h1_size = mlp_h1_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = EncoderNN(input_size, encoder_hidden_size, device)\n",
    "        self.mlp = MLPClassifier(input_size, mlp_h0_size, mlp_h1_size, 2)\n",
    "        self.positive_decoder = DecoderNN(input_size, encoder_hidden_size, input_size, device)\n",
    "        self.negative_decoder = DecoderNN(input_size, encoder_hidden_size, input_size, device)\n",
    "    \n",
    "    def forward(X, y):\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(eldmitro): Make it working\n",
    "def train(\n",
    "    dataloader\n",
    "    encoder_config,\n",
    "    style_classifier_config,\n",
    "    decoder_config,\n",
    "):\n",
    "    encoder, encoder_opt = encoder_config\n",
    "    style_classifier, style_classifier_opt, criteria = style_classifier_config\n",
    "    decoder, decoder_opt, decoder_criteria = decoder_config\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(X_tensor.size(0))\n",
    "    \n",
    "    encoder_opt.zero_grad()\n",
    "    style_classifier_opt.zero_grad()\n",
    "    pos_decoder_opt.zero_grad()\n",
    "    neg_decoder_opt.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    content, hidden = encoder.forward(X_tensor)\n",
    "    output = style_classifier.forward(content)\n",
    "    loss += criteria1(output, y_tensor)\n",
    "    loss += criteria2(output, y_tensor)\n",
    "     \n",
    "    for i in range(X_tensor.size(1)):\n",
    "        d_output, d_hidden = decoder.forward(d_input, content)\n",
    "        topv, topi = d_output.topk(1)\n",
    "        loss += decoder_crit(d_output, X_tensor[0, i, :, :])\n",
    "        \n",
    "    loss.backward()\n",
    "    encoder_opt.step()\n",
    "    style_classifier_opt.step()\n",
    "    decoder_opt.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
